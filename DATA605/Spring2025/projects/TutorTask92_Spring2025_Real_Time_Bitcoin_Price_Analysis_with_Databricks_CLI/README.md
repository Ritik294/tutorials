
# Real-Time Bitcoin Price Analysis using Databricks CLI

**Author**: Ritik Pratap Singh  
**Date**: 2025-04-30  
**Course**: DATA605 - Spring 2025  

## Project Overview

This project implements a pipeline to fetch real-time Bitcoin price data from the CoinGecko API, perform time-series forecasting using an ARIMA model on Databricks compute, and orchestrate the entire workflow using the Databricks Command Line Interface (CLI).

The primary demonstration uses a Jupyter notebook (`databricks_cli.example.ipynb`) running within a Docker container to manage the pipeline steps: creating a Databricks cluster, fetching data, uploading it to DBFS, submitting the analysis notebook as a run on the cluster, monitoring completion, downloading results, and cleaning up the cluster. An auxiliary notebook (`databricks_cli.API.ipynb`) demonstrates the core Databricks CLI commands through Python wrappers.

---

## Project Files

This project contains the following key files and directories:

- `README.md`: This file.
- `databricks_cli_utils.py`: Python module containing helper functions for interacting with the Databricks CLI (via subprocess), fetching data, parsing data, running ARIMA analysis, and plotting.
- `databricks_cli.API.ipynb`: Jupyter notebook demonstrating basic usage of Databricks CLI commands (cluster management, DBFS, job submission) via the Python wrappers. Requires Databricks CLI configuration to run live cells.
- `databricks_cli.API.md`: Markdown documentation explaining the Databricks CLI commands and the corresponding Python wrapper functions. Includes prerequisite setup steps.
- `databricks_cli.example.ipynb`: Jupyter notebook orchestrating the end-to-end Bitcoin analysis pipeline (cluster creation, data fetch/upload, remote job submission via `runs submit`, monitoring, results download, cluster termination, local visualization). Requires Databricks CLI configuration to run.
- `databricks_cli.example.md`: Markdown documentation describing the Bitcoin analysis project workflow and architecture.
- `bitcoin_analysis.ipynb`: (Not included in repo, must be uploaded to Databricks Workspace) The analysis notebook containing Python code (using Pandas, Statsmodels) to load data from DBFS, perform ARIMA forecasting, and save results back to DBFS.

### Folders and Scripts

- `config/`:
  - `cluster_config.json`: JSON specification for the Databricks cluster to be created by the CLI. (Needs to be created by the user)
  - `cluster_id.txt`: (Generated by scripts) Stores the ID of the created cluster.

- `data/`:  
  Directory for local data storage (e.g., fetched `bitcoin_price.json`, downloaded `forecast_output_live.csv`). (Generated by scripts)

- `output_plots/`:  
  Directory where generated plots are saved locally. (Generated by scripts)

- `scripts/`:  
  Directory containing supplementary shell scripts:
  - `run_pipeline.sh`: Optional end-to-end pipeline automation using direct CLI commands and `jobs run-now`.
  - (May include `create_cluster.sh`, `upload_to_dbfs.sh`, etc.)

- `requirements.txt`: Lists necessary Python packages for the environment.
- `Dockerfile`: Defines the Docker image using the `data605_style` approach.
- `docker_build.sh`: Shell script to build the Docker image.
- `docker_bash.sh`: Shell script to start an interactive bash session inside the Docker container.
- `docker_jupyter.sh`: Shell script to start a JupyterLab server inside the Docker container.
- `docker_name.sh`: Defines the name for the Docker image.

---

## Setup and Dependencies

### Prerequisites

- Git: To clone the repository.
- Docker: Docker Desktop (Windows/Mac) or Docker Engine (Linux) must be installed and running.
- Databricks Account: Azure or AWS/GCP-based Databricks workspace access.
- Databricks PAT: Personal Access Token from your Databricks User Settings.
- Python 3.8+

---

## Cloning the Repository

```bash
git clone <your_repository_url>
cd <project_directory_name>  # e.g., TutorTask92_..._Databricks_CLI
```

---

## Building and Running the Docker Container (data605_style)

### Build the Docker Image

```bash
chmod +x *.sh         # If needed
./docker_build.sh
```

### Run a Bash Shell in the Container

```bash
./docker_bash.sh
```

Project directory is mounted at `/app` inside the container. Use `exit` to leave.

### Run JupyterLab Inside the Container

```bash
./docker_jupyter.sh
```

Open the URL (http://127.0.0.1:8888/lab?token=...) in your browser. Use `Ctrl+C` to stop.

---

## Environment Setup (Inside Docker)

### Configure Databricks CLI

```bash
./docker_bash.sh
databricks configure --token
```

Provide your host URL and PAT. Only needed once per container setup.

---

## Prepare Databricks Workspace

1. Upload `bitcoin_analysis.ipynb` to your Databricks Workspace.
2. Copy its full workspace path (e.g., `/Users/your_email@domain.com/bitcoin_analysis`).
3. Set `ANALYSIS_NOTEBOOK_PATH` variable in `databricks_cli.example.ipynb` accordingly.
4. Ensure `config/cluster_config.json` exists and is correctly set up.

### Optional: Create Test Job for API Notebook

To test `jobs run-now`, create a simple test job in Databricks and update the `JOB_ID` variable in `databricks_cli.API.ipynb`.

---

## Usage Guide

### Run Jupyter

```bash
./docker_jupyter.sh
```

Access Jupyter in your browser via the provided URL.

### Run API Demo Notebook

Open `databricks_cli.API.ipynb`  
- Read markdown cells  
- Run command demo cells sequentially (requires CLI setup)

### Run Pipeline Notebook

Open `databricks_cli.example.ipynb`  
- Set `ANALYSIS_NOTEBOOK_PATH`  
- Run all cells  
- Pipeline includes: cluster creation → data fetch/upload → notebook run → result download → cluster termination → local plot

---

### Alternative: Run Shell Script Pipeline

```bash
./docker_bash.sh
./scripts/run_pipeline.sh
```

Requires a valid Job ID and CLI setup.

---

## References

- [Databricks CLI Documentation](https://docs.databricks.com/en/dev-tools/cli/index.html)  
- [CoinGecko API](https://www.coingecko.com/en/api/documentation)  
- [Statsmodels ARIMA](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html)


